{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e8aa710",
   "metadata": {},
   "source": [
    "# Les C7: Time series forecasting  \n",
    "In deze les zien we hoe je _time series forecasting_ aanpakt. Dit doen we aan de hand van ARIMA-modellen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4851c1a5",
   "metadata": {},
   "source": [
    "## Benodigde libraries, functies en data  \n",
    "Zoals altijd beginnen we met het importeren van het een en ander. Allereerst de libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7aac3d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importeer libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8115b913",
   "metadata": {},
   "source": [
    "Hieronder volgt een aantal code chunks met functies die specifiek voor de situaties in deze notebook goed werken. **Houd er rekening mee dat ze dus niet in alle situaties goed hoeven te werken!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec403140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functie voor plotten van time series\n",
    "def plot_series(df, series_name, lags=40, diff=0, seasonal_diff=0, seasonal_period=12):\n",
    "    \"\"\"\n",
    "    Plot a time series alongside its ACF and PACF with Bartlett bounds.\n",
    "    Includes options for regular and seasonal differencing.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing multiple time series as columns.\n",
    "    series_name : str\n",
    "        Column name of the series to plot.\n",
    "    lags : int\n",
    "        Number of lags for ACF/PACF.\n",
    "    diff : int, default=0\n",
    "        Number of regular differences to apply.\n",
    "    seasonal_diff : int, default=0\n",
    "        Number of seasonal differences to apply.\n",
    "    seasonal_period : int, default=12\n",
    "        Seasonal period (e.g., 12 for monthly data with yearly seasonality).\n",
    "    \"\"\"\n",
    "    series = df[series_name]\n",
    "\n",
    "    # Apply differencing\n",
    "    for _ in range(diff):\n",
    "        series = series.diff()\n",
    "    for _ in range(seasonal_diff):\n",
    "        series = series.diff(seasonal_period)\n",
    "    series = series.dropna()\n",
    "\n",
    "    # Compute ACF/PACF (drop lag 0)\n",
    "    acf_vals = acf(series, nlags=lags, fft=False)[1:]\n",
    "    pacf_vals = pacf(series, nlags=lags, method=\"ywm\")[1:]\n",
    "    lags_range = np.arange(1, len(acf_vals)+1)\n",
    "\n",
    "    # Bartlett bounds\n",
    "    n = len(series)\n",
    "    conf = 1.96 / np.sqrt(n)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "    # Time series\n",
    "    axes[0].plot(series.index, series.values, color=\"steelblue\")\n",
    "    axes[0].set_title(f\"Series: {series_name} (diff={diff}, seas_diff={seasonal_diff})\")\n",
    "\n",
    "    # ACF\n",
    "    axes[1].stem(lags_range, acf_vals, basefmt=\" \")\n",
    "    axes[1].hlines([conf, -conf], xmin=0, xmax=lags, colors=\"red\", linestyles=\"dashed\")\n",
    "    axes[1].axhline(0, color=\"black\", linewidth=0.8)\n",
    "    axes[1].set_title(\"ACF-plot\")\n",
    "\n",
    "    # PACF\n",
    "    axes[2].stem(lags_range, pacf_vals, basefmt=\" \")\n",
    "    axes[2].hlines([conf, -conf], xmin=0, xmax=lags, colors=\"red\", linestyles=\"dashed\")\n",
    "    axes[2].axhline(0, color=\"black\", linewidth=0.8)\n",
    "    axes[2].set_title(\"PACF-plot\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c17b3e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functie voor maken van time series cross-validation splits\n",
    "def make_time_series_splits(series, initial_window, horizon=1, step=1, window_type=\"expanding\"):\n",
    "    \"\"\"\n",
    "    Generate time-series cross-validation splits.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    series : pd.Series\n",
    "        Time series (DateTimeIndex or RangeIndex).\n",
    "    initial_window : int\n",
    "        Number of observations in the first training window.\n",
    "    horizon : int\n",
    "        Forecast horizon (steps ahead).\n",
    "    step : int\n",
    "        How many steps to move the origin each iteration.\n",
    "    window_type : str\n",
    "        \"expanding\"  -> training window grows over time\n",
    "        \"sliding\"    -> training window has fixed size\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    splits : list of tuples\n",
    "        Each tuple is (train_idx, test_idx), where each is an array of positions.\n",
    "    \"\"\"\n",
    "    n = len(series)\n",
    "    splits = []\n",
    "\n",
    "    start = initial_window\n",
    "\n",
    "    while start + horizon <= n:\n",
    "        if window_type == \"expanding\":\n",
    "            # Training window always starts at 0\n",
    "            train_idx = np.arange(0, start)\n",
    "\n",
    "        elif window_type == \"sliding\":\n",
    "            # Fixed-size training window\n",
    "            train_idx = np.arange(start - initial_window, start)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"window_type must be 'expanding' or 'sliding'\")\n",
    "\n",
    "        test_idx = np.arange(start, start + horizon)\n",
    "        splits.append((train_idx, test_idx))\n",
    "\n",
    "        start += step\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3f1f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gebruikelijke evaluation metrics voor time series forecasting\n",
    "def mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    return 100 * np.mean(np.abs(y_pred - y_true) / (np.abs(y_true))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f8cf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functies voor time series cross-validation\n",
    "def cv_single_arima(series, order, seasonal_order, splits, metric=rmse):\n",
    "    \"\"\"\n",
    "    Evaluate one ARIMA specification across CV splits.\n",
    "    \n",
    "    Returns the average error across folds.\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "\n",
    "    for train_idx, test_idx in splits:\n",
    "        train = series.iloc[train_idx]\n",
    "        test = series.iloc[test_idx]\n",
    "\n",
    "        model = ARIMA(train, order=order, seasonal_order=seasonal_order)\n",
    "        fitted = model.fit()\n",
    "\n",
    "        fc = fitted.forecast(steps=len(test))\n",
    "        error = metric(test.values, fc.values)\n",
    "        errors.append(error)\n",
    "\n",
    "    return np.mean(errors)\n",
    "\n",
    "def cv_arima_candidates(series, candidates, splits, metric=rmse):\n",
    "    \"\"\"\n",
    "    Evaluate multiple ARIMA candidates.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    candidates : list of dicts\n",
    "        Example:\n",
    "        [\n",
    "            {\"order\": (1,1,1), \"seasonal_order\": (0,1,1,12)},\n",
    "            {\"order\": (2,1,0), \"seasonal_order\": (1,1,0,12)}\n",
    "        ]\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for spec in candidates:\n",
    "        avg_error = cv_single_arima(\n",
    "            series,\n",
    "            order=spec[\"order\"],\n",
    "            seasonal_order=spec[\"seasonal_order\"],\n",
    "            splits=splits,\n",
    "            metric=metric\n",
    "        )\n",
    "        results.append({\n",
    "            \"order\": spec[\"order\"],\n",
    "            \"seasonal_order\": spec[\"seasonal_order\"],\n",
    "            \"cv_error\": avg_error\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b05f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functies voor visualiseren fouten\n",
    "def horizon_errors(series, order, seasonal_order, H, splits, metric):\n",
    "    \"\"\"\n",
    "    Compute horizon-wise forecast errors for ARIMA using any CV splits.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    series : pd.Series\n",
    "    order : tuple\n",
    "    seasonal_order : tuple\n",
    "    H : int\n",
    "        Maximum forecast horizon.\n",
    "    splits : list of (train_idx, test_idx)\n",
    "        Output of make_time_series_splits (expanding or sliding).\n",
    "    \"\"\"\n",
    "    errors = {h: [] for h in range(1, H+1)}\n",
    "\n",
    "    for train_idx, test_idx in splits:\n",
    "        train = series.iloc[train_idx]\n",
    "        test = series.iloc[test_idx]\n",
    "\n",
    "        model = ARIMA(train, order=order, seasonal_order=seasonal_order)\n",
    "        fitted = model.fit()\n",
    "\n",
    "        fc = fitted.forecast(steps=H)\n",
    "\n",
    "        for h in range(1, H+1): \n",
    "            e = metric(test.iloc[h-1], fc.iloc[h-1]) \n",
    "            errors[h].append(e)\n",
    "\n",
    "    avg_errors = {h: np.mean(errors[h]) for h in errors}\n",
    "    return avg_errors, errors\n",
    "\n",
    "def plot_horizon_boxplot(all_errors):\n",
    "    \"\"\"\n",
    "    Boxplot of horizon-wise forecast errors.\n",
    "    \"\"\"\n",
    "    horizons = list(all_errors.keys())\n",
    "    data = [all_errors[h] for h in horizons]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.boxplot(data, labels=horizons, showfliers=True)\n",
    "    plt.xlabel(\"Forecast horizon (steps ahead)\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.title(\"Forecast error distribution\")\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7924cea2",
   "metadata": {},
   "source": [
    "Tenslotte nog wat code waarmee we de dataset inlezen. Hierbij gaan we ervan uit dat je `nl_airports.csv` op de juiste plaats hebt gezet, zodat Python dat bestand kan vinden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dff213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset inlezen\n",
    "# let op het scheidingsteken\n",
    "nl_airports = pd.read_csv('nl_airports.csv', sep = ';')\n",
    "\n",
    "# maak van Month een datetime-object\n",
    "nl_airports['Month'] = pd.to_datetime(nl_airports['Month'], format=\"%Y %m\")\n",
    "\n",
    "# hernoem kolommen van vliegvelden\n",
    "nl_airports = nl_airports.rename(columns={\"Amsterdam Airport Schiphol\": \"AMS\", \"Rotterdam The Hague Airport\": \"RTM\", \"Eindhoven Airport\": \"EIN\", \"Maastricht Aachen Airport\": \"MST\", \"Groningen Airport Eelde\": \"GRQ\"})\n",
    "\n",
    "# maak van Month de index en sorteer deze\n",
    "nl_airports = nl_airports.set_index('Month').sort_index()\n",
    "\n",
    "# geef de frequentie van de index aan\n",
    "nl_airports = nl_airports.asfreq(\"MS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952be7d1",
   "metadata": {},
   "source": [
    "## Selecteren van kandidaatmodellen  \n",
    "We richten ons op de passagiersaantallen op Schiphol, in deze dataset nu aangeduid met `AMS`. In de onderstaande code zie je dat een ARIMA(0,1,0)(0,1,1)$_{12}$ is gefit op deze time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a01a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit SARIMA-model met (p,d,q)=(0,1,0), (P,D,Q)=(0,1,1) en m=12\n",
    "model = ARIMA(nl_airports[\"AMS\"], order=(0,1,0), seasonal_order=(0,1,1,12))\n",
    "fitted = model.fit()\n",
    "\n",
    "# plot gefitte ARIMA-model met de time series\n",
    "plt.plot(nl_airports.index, fitted.fittedvalues, label=\"ARIMA(0,1,0)(0,1,1)[12]\", color=\"darkorange\")\n",
    "plt.plot(nl_airports.index, nl_airports['AMS'], label=\"AMS\", color=\"steelblue\")\n",
    "plt.title(\"AMS and fitted SARIMA-model\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc5773c",
   "metadata": {},
   "source": [
    "**_Opgave 1_**  \n",
    "Run bovenstaande code en ga na dat het model in het begin nog veel moeite heeft met het goed fitten op de time series. Heb je daar een verklaring voor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360ce8ad",
   "metadata": {},
   "source": [
    "**_Opgave 2_**  \n",
    "Vind je ARIMA(0,1,0)(0,1,1)$_{12}$ een goede keuze aan parameters voor deze time series?  \n",
    "Zo ja, waarom? Hoe kan je dat nagaan?  \n",
    "Zo nee, waarom niet? Wat zouden volgens jou wel goede parameters zijn?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0de9daf",
   "metadata": {},
   "source": [
    "In les C5 en C6 hebben we gezien dat het selecteren van parameters voor ARIMA-modellen deels een subjectieve gelegenheid is. Dat betekent dat er niet een exacte manier is om van tevoren na te gaan of je de \"beste\" combinatie van parameters (wat dat dan ook betekent) hebt gevonden. Voor voorspellingsdoeleinden zou je daarentegen juist wel graag die zekerheid willen. Maar omdat dat niet van tevoren kan, kunnen we dat alleen maar achteraf nagaan.  \n",
    "\n",
    "In theorie zou je dus alle mogelijke parametercombinaties kunnen afgaan, net zolang tot je de beste gevonden hebt. Zo'n uitputtende grid search werkt zeker, maar leidt al heel snel tot overfitte modellen (zelfs met de nodige regularisatie). Om die reden heeft een uitputtende grid search voor ARIMA niet de voorkeur.  \n",
    "\n",
    "In plaats daarvan kiezen we voor een slimmere grid search, eentje die een stuk meer afgebakend is. Deze werkt als volgt:  \n",
    "1. Kies eerst één combinatie van geschikte parameterwaarden $(p,d,q)(P,D,Q)_m$ aan de hand van (P)ACF-plots.  \n",
    "2. Maak vanuit deze combinatie meerdere mogelijke combinaties door de waarden van $p,q,P,Q$ allemaal met $1$ te verhogen en te verlagen. Zodoende krijg je nu maximaal 81 kandidaatmodellen.  \n",
    "3. Verwijder alle kandidaatmodellen waarbij minstens één van $p,q,P,Q$ negatief is, en ook waarbij $p+q$ en/of $P+Q$ groot zijn.  \n",
    "\n",
    "Na stap 3 houd je een kleinere selectie aan combinaties over. Dit zijn de kandidaatmodellen waar je verder mee aan de slag gaat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ab6741",
   "metadata": {},
   "source": [
    "**_Opgave 3_**  \n",
    "Ga uit van de parametercombinatie die je bij Opgave 2 hebt gekozen. Bepaal vanuit deze combinatie de kandidaatmodellen waar jij mee verder wilt gaan. Mik in eerste instantie op **niet meer dan 10** kandidaatmodellen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c0ff04",
   "metadata": {},
   "source": [
    "## Time series forecasting\n",
    "### Waarom time series cross-validation anders is  \n",
    "Het idee van voorspellen en 'probeer veel modellen en bekijk daarna welke het beste presteert', doet je wellicht denken aan data science. En inderdaad, we gaan hier ook een data science-aanpak hanteren. Om precies te zijn, het principe van _cross-validation_: we houden data apart achter waar het model niet op traint, zodat we later kunnen nagaan of het model inderdaad goed in staat is om te voorspellen op nieuwe data.  \n",
    "\n",
    "Toch gaat cross-validation niet op de standaard manier zoals je die kent van data science. En het verschil komt door de index: er zit namelijk een volgorde in de tijd. _Forecasting_ kan je zien als het voorspellen van de time series voor toekomstige waarden van de index. Dat betekent dat we dat aspect - het voorspellen bij toekomstige index-waarden - ook moeten nabootsen in cross-validation. En dat lukt niet met een _random shuffle_, zoals dat gebruikelijk is. Voor een _time series cross-validation_ geldt dat de test set altijd chronologisch na de training set komt en nooit andersom.  \n",
    "\n",
    "### Trainen en testen: time series jargon  \n",
    "In de wereld van _time series forecasting_ bezigen we specifieke jargon, waarvan het goed is dat je die kent.  \n",
    "* De _horizon_ is de lengte van de periode die je probeert te voorspellen. Als je een tijdreeks op maandbasis hebt en je wilt 1 jaar vooruit voorspellen, dan is je horizon dus 12 maanden.  \n",
    "* De _test set_ is een gedeelte van de waargenomen time series dat gebruikt wordt om de prestaties van het voorspellingsmodel mee te meten. In de regel is de even groot als de horizon.  \n",
    "* De _training set_ is een gedeelte van de waargenomen time series dat gebruikt wordt om het voorspellingsmodel op te trainen. De training set zit, chronologisch gezien, altijd voorafgaand aan de test set. Verder bevat de training set minstens 2 keer zo veel datapunten als groottes van time series componenten die je in het model wilt meenemen. Als je time series een seasonality met periode 7 heeft, moet je training set uit minstens 14 datapunten bestaan om het model de seasonality te laten leren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f6f37e",
   "metadata": {},
   "source": [
    "**_Opgave 4_**  \n",
    "Stel, je wilt een ARIMA-model gebruiken om een time series te forecasten. De time series bevat een trend en een seasonality met periode 16. Je hebt aan twee keer regular differencing en één keer seasonal differencing genoeg om de time series stationary te maken. De beoogde horizon is 8 tijdseenheden.  \n",
    "Toon aan dat je dan minstens 44 datapunten moet hebben om time series forecasting te kunnen uitvoeren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13633833",
   "metadata": {},
   "source": [
    "In Opgave 4 heb je eigenlijk alleen maar gekeken naar het absolute minimum aan datapunten zodat het technisch gezien mogelijk is om time series forecasting te kunnen doen. Maar dat maakt het nog zeker geen goede forecasting; daarvoor wil je toch echt wel meer datapunten hebben. Hoeveel meer, daar zijn geen harde regels voor; dat hangt namelijk ook heel sterk af van de context (wat stelt de time series in de praktijk voor). Een grove vuistregel (maar dus zeker geen exacte regel) is dat je minstens 5 seasonalities in je training set wilt hebben zitten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d63c520",
   "metadata": {},
   "source": [
    "**_Opgave 5_**  \n",
    "Voor de passagiersaantallen op Schiphol willen we een voorspelling maken voor 1 jaar vooruit.  \n",
    "Hebben we genoeg data voor een potentieel goede time series forecasting? Waarom?  \n",
    "Hoe groot moet de training set zijn? En de test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4c963f",
   "metadata": {},
   "source": [
    "## Time series cross-validation  \n",
    "Time series cross-validation is dus, zoals eerder benoemd, niet hetzelfde als de gewone cross-validation (CV). Net als bij gewone cross-validation willen we meerdere train-testsplits kunnen maken, om zo een beter beeld te krijgen van de prestaties van het model. Het gebruik van meerdere splits maakt de kans dat er in onze resultaten een _bias_ zit, minder groot. We moeten wel goed nadenken hoe we aan die meerdere splits komen, aangezien we wel de chronologische volgorde (eerst training, daarna test) steeds in stand moeten houden. Grofweg zijn er twee varianten van time series cross-validation:  \n",
    "* _rolling window cross-validation_ (ook wel _expanding window cross-validation_ genoemd): In de eerste fold bestaat de training set uit de eerste waarnemingen van de time series, zo groot als nodig is. Daarachter zit de test set geplakt, even groot als de horizon. In elk daaropvolgende fold wordt de training set uitgebreid met $k$ tijdstappen, en schuift de test set met $k$ tijdstappen op. Hierdoor geldt in elke fold dat de testset steeds direct volgt op de training set.  \n",
    "* _sliding window cross-validation_: De eerste fold is identiek aan die bij rolling window cross-validation. Maar in elke daaropvolgende fold schuift de training set in zijn geheel op met $k$ tijdstappen (en de test set dus ook). Het verschil is dat hier de training set altijd dezelfde grootte heeft, ongeacht naar welke fold je kijkt.  \n",
    "\n",
    "Beide varianten worden in de praktijk gebruikt en er lijkt geen consensus te zijn over welke variant beter is. Rolling window CV is wat natuurlijker, in die zin dat in de praktijk vaak zoveel mogelijk data gebruikt wordt voor voorspellingsdoeleinden. Het idee van groeiende training sets sluit daar beter op aan. Sliding window CV is daarentegen weer wat interessanter op het moment dat je teveel data kan hebben (bijvoorbeeld omdat het trainingsproces dan te lang duurt). Sliding window CV maakt het ook wat makkelijker/eerlijker om verschillende folds met elkaar te vergelijken en zodoende na te gaan of modellen moeite hebben met specifieke stukken van de time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e556fb4c",
   "metadata": {},
   "source": [
    "**_Opgave 6_**  \n",
    "Voor de passagiersaantallen op Schiphol willen we een voorspelling maken voor 1 jaar vooruit.  \n",
    "Gebruik de functie `make_time_series_splits()` om de splits voor een rolling window cross-validation te maken, waarbij je de training set steeds met 5 maanden uitbreidt.  \n",
    "Kijk goed naar de functiebeschrijving bovenaan deze notebook, om te zien welke argumenten de functie heeft. En vergeet je antwoord bij Opgave 5 niet!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56ab0a2",
   "metadata": {},
   "source": [
    "**_Opgave 7_**  \n",
    "Wat gaat er mis als je in Opgave 6 de training set zou uitbreiden met 12 maanden?  \n",
    "Zou dat wel goed gaan bij 24 maanden? En bij 3 maanden?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677fcd4b",
   "metadata": {},
   "source": [
    "**_Opgave 8_**  \n",
    "Zet de door jou bij Opgave 3 gekozen kandidaatmodellen in een dictionary. Hieronder is een beginnetje gemaakt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19215774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary van kandidaatmodellen\n",
    "candidates = [{\"order\": (0,1,0), \"seasonal_order\": (0,1,1,12)}, \n",
    "              ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9488f1f6",
   "metadata": {},
   "source": [
    "**_Opgave 9_**  \n",
    "Voer de rolling window CV uit, met behulp van jouw antwoorden bij Opgave 6, Opgave 8 en de functie `cv_arima_candidates()`.  \n",
    "Welk model scoort het beste aan de hand van de MAPE?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0cf3d8",
   "metadata": {},
   "source": [
    "**_Opgave 10_**  \n",
    "Plot de voorspelling voor het jaar 2019 volgens het model dat bij Opgave 9 het beste was."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150b6d28",
   "metadata": {},
   "source": [
    "### Hoe goed is het model nu echt?  \n",
    "Uit Opgave 9 komt weliswaar het beste van de kandidaatmodellen naar voren, maar daaruit maken we nog niet op hoe goed dat model nu echt in staat is om te voorspellen. Een logische gedachte is dat hoe verder vooruit je probeert te voorspellen, hoe groter de fouten zullen zijn. Immers, je stapelt onzekerheid op onzekerheid, waardoor fouten ook potentieel groter kunnen worden. De meeste evaluation metrics als RMSE, MAE en MAPE verhullen dat effect, omdat ze het gemiddelde van alle fouten over de hele horizon nemen.\n",
    "\n",
    "Een visualisatie waarmee je de groei van de voorspellingsfouten in de horizon kan laten zien, wordt ook wel de _forecast error distribution-plot_ genoemd. Er zijn meerdere manieren om zo'n plot weer te geven. De functie `plot_horizon_boxplot()` doet dit aan de hand van boxplots per tijdstap in de horizon. De keuze voor boxplots is dat je dan ook nog onderscheid kan maken tussen toevallige uitschieters (die wellicht door _time series outliers_ zijn veroorzaakt) en systematische foutengroei."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be727890",
   "metadata": {},
   "source": [
    "**_Opgave 11_**  \n",
    "Maak een forecast error distribution-plot bij het model dat bij Opgave 9 het beste was. Gebruik de functies `horizon_errors()` en `plot_horizon_boxplot()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba4a9f",
   "metadata": {},
   "source": [
    "## Waarom ARIMA?  \n",
    "Voor een heel groot deel is de methodiek van deze les niet afhankelijk van het feit dat we met ARIMA-modellen werken. Dat betekent dat je dezelfde werkwijze ook kan gebruiken voor andere soorten modellen. Met de nodige _feature-engineering_ kan je ook de modellen van DS2 en DS3 gebruiken voor time series forecasting.  \n",
    "\n",
    "Achter het gebruik van ARIMA-modellen schuilt wel een centrale aanname: je neemt aan dat algemene kenmerken van de time series (namelijk, de autocorrelaties) ook representatief zijn voor de nabije toekomst. Maar dat hoeft natuurlijk niet zo te zijn. Zeker bij time series die sterk afhankelijk zijn van externe invloeden, geldt dat data science-modellen daar beter mee kunnen omgaan; die werken namelijk niet vanuit de centrale aanname waar ARIMA op gestoeld is."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
